"use strict";(self.webpackChunkopen_assistant=self.webpackChunkopen_assistant||[]).push([[191],{3905:(e,n,t)=>{t.d(n,{Zo:()=>m,kt:()=>d});var a=t(7294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=a.createContext({}),u=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},m=function(e){var n=u(e.components);return a.createElement(l.Provider,{value:n},e.children)},c="mdxType",h={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},g=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),c=u(t),g=r,d=c["".concat(l,".").concat(g)]||c[g]||h[g]||i;return t?a.createElement(d,o(o({ref:n},m),{},{components:t})):a.createElement(d,o({ref:n},m))}));function d(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var i=t.length,o=new Array(i);o[0]=g;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[c]="string"==typeof e?e:r,o[1]=s;for(var u=2;u<i;u++)o[u]=t[u];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}g.displayName="MDXCreateElement"},8604:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>s,toc:()=>u});var a=t(7462),r=(t(7294),t(3905));const i={},o="Research",s={unversionedId:"research/general",id:"research/general",title:"Research",description:"This page lists research papers that are relevant to the project.",source:"@site/docs/research/general.md",sourceDirName:"research",slug:"/research/general",permalink:"/Open-Assistant/docs/research/general",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Research",permalink:"/Open-Assistant/docs/research/"},next:{title:"Cohere Grounded QA",permalink:"/Open-Assistant/docs/research/search-based-qa"}},l={},u=[{value:"Table of Contents",id:"table-of-contents",level:2},{value:'Reinforcement Learning from Human Feedback <a name="reinforcement-learning-from-human-feedback"></a>',id:"reinforcement-learning-from-human-feedback-",level:2},{value:"Learning to summarize from human feedback [ArXiv], [Github]",id:"learning-to-summarize-from-human-feedback-arxiv-github",level:3},{value:"Training language models to follow instructions with human feedback [ArXiv]",id:"training-language-models-to-follow-instructions-with-human-feedback-arxiv",level:3},{value:"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback [ArXiv]",id:"training-a-helpful-and-harmless-assistant-with-reinforcement-learning-from-human-feedback-arxiv",level:3},{value:"Generating Text From Language Models",id:"generating-text-from-language-models",level:2},{value:"RANKGEN: Improving Text Generation with Large Ranking Models [ArXiv], [Github]",id:"rankgen-improving-text-generation-with-large-ranking-models-arxiv-github",level:3},{value:"Automatically Generating Instruction Data for Training",id:"automatically-generating-instruction-data-for-training",level:2},{value:"SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions [ArXiv], [Github].",id:"self-instruct-aligning-language-model-with-self-generated-instructions-arxiv-github",level:3},{value:"Tuning Language Models with (Almost) No Human Labor. [ArXiv], [Github].",id:"tuning-language-models-with-almost-no-human-labor-arxiv-github",level:3},{value:"Uncertainty Estimation of Language Model Outputs",id:"uncertainty-estimation-of-language-model-outputs",level:2},{value:"Teaching models to express their uncertainty in words [Arxiv]",id:"teaching-models-to-express-their-uncertainty-in-words-arxiv",level:3}],m={toc:u};function c(e){let{components:n,...t}=e;return(0,r.kt)("wrapper",(0,a.Z)({},m,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"research"},"Research"),(0,r.kt)("p",null,"This page lists research papers that are relevant to the project."),(0,r.kt)("h2",{id:"table-of-contents"},"Table of Contents"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Reinforcement Learning from Human Feedback"),(0,r.kt)("li",{parentName:"ul"},"Generating Text From Language Models"),(0,r.kt)("li",{parentName:"ul"},"Automatically Generating Instruction Data for Training"),(0,r.kt)("li",{parentName:"ul"},"Uncertainty Estimation of Language Model Outputs")),(0,r.kt)("h2",{id:"reinforcement-learning-from-human-feedback-"},"Reinforcement Learning from Human Feedback ",(0,r.kt)("a",{name:"reinforcement-learning-from-human-feedback"})),(0,r.kt)("p",null,"Reinforcement Learning from Human Feedback (RLHF) is a method for fine-tuning a\ngenerative language models based on a reward model that is learned from human\npreference data. This method facilitates the learning of instruction-tuned\nmodels, among other things."),(0,r.kt)("h3",{id:"learning-to-summarize-from-human-feedback-arxiv-github"},"Learning to summarize from human feedback [",(0,r.kt)("a",{parentName:"h3",href:"https://arxiv.org/pdf/2009.01325.pdf"},"ArXiv"),"], [",(0,r.kt)("a",{parentName:"h3",href:"https://github.com/openai/summarize-from-feedback"},"Github"),"]"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"In this work, we show that it is possible to significantly improve summary\nquality by training a model to optimize for human preferences. We collect a\nlarge, high-quality dataset of human comparisons between summaries, train a\nmodel to predict the human-preferred summary, and use that model as a reward\nfunction to fine-tune a summarization policy using reinforcement learning.")),(0,r.kt)("h3",{id:"training-language-models-to-follow-instructions-with-human-feedback-arxiv"},"Training language models to follow instructions with human feedback [",(0,r.kt)("a",{parentName:"h3",href:"https://arxiv.org/pdf/2203.02155.pdf"},"ArXiv"),"]"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Starting with a set of labeler-written prompts and prompts submitted through\nthe OpenAI API, we collect a dataset of labeler demonstrations of the desired\nmodel behavior, which we use to fine-tune GPT-3 using supervised learning. We\nthen collect a dataset of rankings of model outputs, which we use to further\nfine-tune this supervised model using reinforcement learning from human\nfeedback.")),(0,r.kt)("h3",{id:"training-a-helpful-and-harmless-assistant-with-reinforcement-learning-from-human-feedback-arxiv"},"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback [",(0,r.kt)("a",{parentName:"h3",href:"https://arxiv.org/pdf/2204.05862.pdf"},"ArXiv"),"]"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"We apply preference modeling and reinforcement learning from human feedback\n(RLHF) to finetune language models to act as helpful and harmless assistants.\nWe find this alignment training improves performance on almost all NLP\nevaluations, and is fully compatible with training for specialized skills such\nas python coding and summarization.")),(0,r.kt)("h2",{id:"generating-text-from-language-models"},"Generating Text From Language Models"),(0,r.kt)("p",null,"A language model generates output text token by token, autoregressively. The\nlarge search space of this task requires some method of narrowing down the set\nof tokens to be considered in each step. This method, in turn, has a big impact\non the quality of the resulting text."),(0,r.kt)("h3",{id:"rankgen-improving-text-generation-with-large-ranking-models-arxiv-github"},"RANKGEN: Improving Text Generation with Large Ranking Models [",(0,r.kt)("a",{parentName:"h3",href:"https://arxiv.org/pdf/2205.09726.pdf"},"ArXiv"),"], [",(0,r.kt)("a",{parentName:"h3",href:"https://github.com/martiansideofthemoon/rankgen"},"Github"),"]"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Given an input sequence (or prefix), modern language models often assign high\nprobabilities to output sequences that are repetitive, incoherent, or\nirrelevant to the prefix; as such, model-generated text also contains such\nartifacts. To address these issues we present RankGen, a 1.2B parameter\nencoder model for English that scores model generations given a prefix.\nRankGen can be flexibly incorporated as a scoring function in beam search and\nused to decode from any pretrained language model.")),(0,r.kt)("h2",{id:"automatically-generating-instruction-data-for-training"},"Automatically Generating Instruction Data for Training"),(0,r.kt)("p",null,"This line of work is about significantly reducing the need for manually\nannotated data for the purpose of training\n",(0,r.kt)("a",{parentName:"p",href:"https://openai.com/blog/instruction-following/"},"instruction-aligned")," language\nmodels."),(0,r.kt)("h3",{id:"self-instruct-aligning-language-model-with-self-generated-instructions-arxiv-github"},"SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions [",(0,r.kt)("a",{parentName:"h3",href:"https://arxiv.org/pdf/2212.10560.pdf"},"ArXiv"),"], [",(0,r.kt)("a",{parentName:"h3",href:"https://github.com/yizhongw/self-instruct"},"Github"),"]."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"We introduce SELF-INSTRUCT, a framework for improving the\ninstruction-following capabilities of pretrained language models by\nbootstrapping off its own generations. Our pipeline generates instruction,\ninput, and output samples from a language model, then prunes them before using\nthem to finetune the original model. Applying our method to vanilla GPT3, we\ndemonstrate a 33% absolute improvement over the original model on\nSuperNaturalInstructions, on par with the performance of InstructGPT-0011,\nwhich is trained with private user data and human annotations.")),(0,r.kt)("h3",{id:"tuning-language-models-with-almost-no-human-labor-arxiv-github"},"Tuning Language Models with (Almost) No Human Labor. [",(0,r.kt)("a",{parentName:"h3",href:"https://arxiv.org/pdf/2212.09689.pdf"},"ArXiv"),"], [",(0,r.kt)("a",{parentName:"h3",href:"https://github.com/orhonovich/unnatural-instructions"},"Github"),"]."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"In this work, we introduce Unnatural Instructions: a large dataset of creative\nand diverse instructions, collected with virtually no human labor. We collect\n64,000 examples by prompting a language model with three seed examples of\ninstructions and eliciting a fourth. This set is then expanded by prompting\nthe model to rephrase each instruction, creating a total of approximately\n240,000 examples of instructions, inputs, and outputs. Experiments show that\ndespite containing a fair amount of noise, training on Unnatural Instructions\nrivals the effectiveness of training on open-source manually-curated datasets,\nsurpassing the performance of models such as T0++ and Tk-Instruct across\nvarious benchmarks.")),(0,r.kt)("h2",{id:"uncertainty-estimation-of-language-model-outputs"},"Uncertainty Estimation of Language Model Outputs"),(0,r.kt)("h3",{id:"teaching-models-to-express-their-uncertainty-in-words-arxiv"},"Teaching models to express their uncertainty in words [",(0,r.kt)("a",{parentName:"h3",href:"https://arxiv.org/pdf/2205.14334.pdf"},"Arxiv"),"]"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},'We show that a GPT-3 model can learn to express uncertainty about its own\nanswers in natural language -- without use of model logits. When given a\nquestion, the model generates both an answer and a level of confidence (e.g.\n"90% confidence" or "high confidence"). These levels map to probabilities that\nare well calibrated. The model also remains moderately calibrated under\ndistribution shift, and is sensitive to uncertainty in its own answers, rather\nthan imitating human examples.')))}c.isMDXComponent=!0}}]);